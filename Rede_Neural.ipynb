{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlVnW8gS3KjaGQkVr61eIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zarkuslol/gsi073-llm/blob/main/Rede_Neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importações"
      ],
      "metadata": {
        "id": "3EsoCcqhWkZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sklearn\n",
        "from torch import nn\n",
        "\n",
        "# Modelo\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cG2F__HBWm_z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregamento dos dados"
      ],
      "metadata": {
        "id": "YWl06A8wWvWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris.data\n",
        "y = (iris.target == 1).astype(float)"
      ],
      "metadata": {
        "id": "wh9rkfStWwzF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).view(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1hXWCZXBZa",
        "outputId": "c668284d-125a-4f26-85aa-dcd602e57b46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2694997208.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype=torch.float32)\n",
            "/tmp/ipython-input-2694997208.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definição do modelo"
      ],
      "metadata": {
        "id": "JYt3sRVJXab9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tyJopCCMV5-s"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criação do modelo"
      ],
      "metadata": {
        "id": "s97URwIOYTfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(4, 8, 1)\n",
        "\n",
        "import copy\n",
        "clone = copy.deepcopy(model)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "495vkhunYTG1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop de treino com SGD"
      ],
      "metadata": {
        "id": "XCwOcbaBY-MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model(X)\n",
        "  loss = loss_function(outputs, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print(f\"Época [{epoch+1}/100], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp1nyvxaY_vu",
        "outputId": "e6641bd1-e014-4754-c151-0e1a311a17bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.4795\n",
            "Época [200/100], Loss: 0.3958\n",
            "Época [300/100], Loss: 0.2987\n",
            "Época [400/100], Loss: 0.2177\n",
            "Época [500/100], Loss: 0.1362\n",
            "Época [600/100], Loss: 0.2743\n",
            "Época [700/100], Loss: 0.0883\n",
            "Época [800/100], Loss: 0.0805\n",
            "Época [900/100], Loss: 0.0754\n",
            "Época [1000/100], Loss: 0.0719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop de treino com otimizador manual"
      ],
      "metadata": {
        "id": "aUArJIcwZqDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()            # Limpa gradientes\n",
        "    outputs = clone(X)               # Forward\n",
        "    loss = loss_function(outputs, y) # Calcula perda\n",
        "    loss.backward()                  # Calcula derivadas do gradiente\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in clone.parameters():\n",
        "            param -= learning_rate * param.grad  # regra de atualização de pesos\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Época [{epoch+1}/1000], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LURebetZobj",
        "outputId": "c0df6433-0f00-4c13-a6a5-39140f44ad90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/1000], Loss: 1.1878\n",
            "Época [200/1000], Loss: 0.9264\n",
            "Época [300/1000], Loss: 0.5037\n",
            "Época [400/1000], Loss: 0.6505\n",
            "Época [500/1000], Loss: 1.0960\n",
            "Época [600/1000], Loss: 1.1328\n",
            "Época [700/1000], Loss: 0.5926\n",
            "Época [800/1000], Loss: 0.8987\n",
            "Época [900/1000], Loss: 1.0938\n",
            "Época [1000/1000], Loss: 0.6815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Avaliar se o modelo aprendido está bom"
      ],
      "metadata": {
        "id": "CqgXCRcyaKjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs_model = model(X)\n",
        "  predicted_model = (torch.sigmoid(outputs_model) > 0.5).float()\n",
        "  accuracy_model = (predicted_model == y).float().mean()\n",
        "  print(f\"Acurácia do modelo com SGD: {accuracy_model.item():.4f}\")\n",
        "\n",
        "  outputs_clone = clone(X)\n",
        "  predicted_clone = (torch.sigmoid(outputs_clone) > 0.5).float()\n",
        "  accuracy_clone = (predicted_clone == y).float().mean()\n",
        "  print(f\"Acurácia do modelo com otimizador manual: {accuracy_clone.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFh0-Js6aNu6",
        "outputId": "d248433a-5312-4fcb-8816-c4e701aa61be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia do modelo com SGD: 0.9867\n",
            "Acurácia do modelo com otimizador manual: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferença na acurácia entre os dois modelos se deve à forma como os gradientes são calculados e aplicados durante o treinamento.\n",
        "\n",
        "O modelo treinado com o `torch.optim.SGD` se beneficia da implementação otimizada do PyTorch para o cálculo e aplicação dos gradientes. O SGD do PyTorch gerencia automaticamente o processo de atualização dos pesos de forma eficiente, garantindo que a regra de atualização (`param -= learning_rate * param.grad`) seja aplicada corretamente a todos os parâmetros do modelo.\n",
        "\n",
        "Já no loop de treinamento manual, embora a regra de atualização seja a mesma, a forma como ela é aplicada pode ser menos eficiente ou estar sujeita a pequenos detalhes de implementação que impactam o desempenho. Além disso, o `torch.optim.SGD` pode ter otimizações internas ou lidar com casos específicos de forma mais robusta do que a implementação manual simples.\n",
        "\n",
        "Em resumo, a diferença de desempenho provavelmente se resume à eficiência e precisão da implementação do SGD no PyTorch em comparação com a aplicação manual da regra de atualização."
      ],
      "metadata": {
        "id": "t9khxhuVbFlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Redução da loss na época 100 de acordo com o aumento de neurônios da camada escondida (hidden_dim)"
      ],
      "metadata": {
        "id": "DghTcsEiecvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_boosted_model(X, y, input_dim=4, hidden_dim=8, output_dim=1):\n",
        "  boosted = NeuralNet(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "  learning_rate_boosted = 0.1\n",
        "\n",
        "  loss_function_boosted = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer_boosted = torch.optim.SGD(boosted.parameters(), lr=learning_rate_boosted)\n",
        "\n",
        "  for epoch in range(1000):\n",
        "    optimizer_boosted.zero_grad()\n",
        "    outputs = boosted(X)\n",
        "    loss = loss_function_boosted(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer_boosted.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      print(f\"Época [{epoch+1}/100], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "So5gt9B3fr1V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_boosted_model(X, y, 4, 9, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfIAzZQtbFQK",
        "outputId": "1e72f7fe-1c6f-4217-f358-a72f9c499155"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.5277\n",
            "Época [200/100], Loss: 0.3862\n",
            "Época [300/100], Loss: 0.2704\n",
            "Época [400/100], Loss: 0.1808\n",
            "Época [500/100], Loss: 0.1229\n",
            "Época [600/100], Loss: 0.0920\n",
            "Época [700/100], Loss: 0.0901\n",
            "Época [800/100], Loss: 0.0725\n",
            "Época [900/100], Loss: 0.0687\n",
            "Época [1000/100], Loss: 0.0670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_boosted_model(X, y, 4, 10, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVnBAZXCe6md",
        "outputId": "20a76a2f-2214-4eec-cd08-ca1a2fd71085"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.4963\n",
            "Época [200/100], Loss: 0.4640\n",
            "Época [300/100], Loss: 0.3997\n",
            "Época [400/100], Loss: 0.3993\n",
            "Época [500/100], Loss: 0.2259\n",
            "Época [600/100], Loss: 0.1479\n",
            "Época [700/100], Loss: 0.1149\n",
            "Época [800/100], Loss: 0.0959\n",
            "Época [900/100], Loss: 0.0911\n",
            "Época [1000/100], Loss: 0.0777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_boosted_model(X, y, 4, 11, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm96RxFugTne",
        "outputId": "b09d71cc-aee6-4edd-cdb0-4742ec8bd594"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.5335\n",
            "Época [200/100], Loss: 0.4412\n",
            "Época [300/100], Loss: 0.3694\n",
            "Época [400/100], Loss: 0.3130\n",
            "Época [500/100], Loss: 0.2524\n",
            "Época [600/100], Loss: 0.1877\n",
            "Época [700/100], Loss: 0.1409\n",
            "Época [800/100], Loss: 0.1118\n",
            "Época [900/100], Loss: 0.0932\n",
            "Época [1000/100], Loss: 0.0871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_boosted_model(X, y, 4, 12, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsDX89LagVwo",
        "outputId": "400b896a-e1b1-4f70-8f1d-b897566fbe0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.5794\n",
            "Época [200/100], Loss: 0.5686\n",
            "Época [300/100], Loss: 0.5558\n",
            "Época [400/100], Loss: 0.5473\n",
            "Época [500/100], Loss: 0.5397\n",
            "Época [600/100], Loss: 0.5336\n",
            "Época [700/100], Loss: 0.5285\n",
            "Época [800/100], Loss: 0.5092\n",
            "Época [900/100], Loss: 0.3995\n",
            "Época [1000/100], Loss: 0.3099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_boosted_model(X, y, 4, 13, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILQ3A-iTgZEa",
        "outputId": "03de7dcd-ee52-48e3-fdd5-130c0a9312bb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.4601\n",
            "Época [200/100], Loss: 0.3486\n",
            "Época [300/100], Loss: 0.3023\n",
            "Época [400/100], Loss: 0.2312\n",
            "Época [500/100], Loss: 0.1582\n",
            "Época [600/100], Loss: 0.1213\n",
            "Época [700/100], Loss: 0.0969\n",
            "Época [800/100], Loss: 0.0873\n",
            "Época [900/100], Loss: 0.0827\n",
            "Época [1000/100], Loss: 0.0805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_boosted_model(X, y, 4, 14, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNgPqVAzgrlb",
        "outputId": "c8c2794c-c72d-4cf8-d337-342aa347bf62"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/100], Loss: 0.5352\n",
            "Época [200/100], Loss: 0.3847\n",
            "Época [300/100], Loss: 0.2493\n",
            "Época [400/100], Loss: 0.1440\n",
            "Época [500/100], Loss: 0.0939\n",
            "Época [600/100], Loss: 0.0842\n",
            "Época [700/100], Loss: 0.0785\n",
            "Época [800/100], Loss: 0.0724\n",
            "Época [900/100], Loss: 0.0690\n",
            "Época [1000/100], Loss: 0.0665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cdcb74"
      },
      "source": [
        "Analisando os resultados do treinamento com diferentes números de neurônios na camada escondida (de 9 a 14), podemos observar a perda (Loss) na época 100 para cada configuração:\n",
        "\n",
        "- `hidden_dim = 8` (modelo original): Loss na época 100: 0.4795\n",
        "- `hidden_dim = 9`: Loss na época 100: 0.5277\n",
        "- `hidden_dim = 10`: Loss na época 100: 0.4963\n",
        "- `hidden_dim = 11`: Loss na época 100: 0.5335\n",
        "- `hidden_dim = 12`: Loss na época 100: 0.5794\n",
        "- `hidden_dim = 13`: Loss na época 100: 0.4601\n",
        "- `hidden_dim = 14`: Loss na época 100: 0.5352\n",
        "\n",
        "Não há uma tendência clara de redução monotônica da perda na época 100 à medida que o número de neurônios na camada escondida aumenta. Os valores de perda na época 100 variam entre as diferentes configurações, e em alguns casos, o aumento do número de neurônios resultou em uma perda maior nessa época específica.\n",
        "\n",
        "É importante notar que a perda na época 100 é apenas um instantâneo do processo de treinamento. O desempenho final do modelo após 1000 épocas pode ser um indicador melhor do impacto do número de neurônios. No entanto, com base apenas na perda na época 100, não podemos afirmar que aumentar os neurônios na camada escondida sempre leva a uma perda menor nesse ponto do treinamento. A otimização de hiperparâmetros, como o número de neurônios, geralmente requer experimentação e avaliação do desempenho final do modelo."
      ]
    }
  ]
}